{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-prepareation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use numpy read file + use torch read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15740229, 3])\n",
      "torch.Size([15740229])\n",
      "0:03:52.921357\n"
     ]
    }
   ],
   "source": [
    "file = \"./data/arch/Train/7_TR_cloister.txt\"\n",
    "time1 = datetime.datetime.now()\n",
    "data = np.loadtxt(file)\n",
    "scene_points = torch.from_numpy(data[:, 0:3].astype('float32'))\n",
    "segment_label = torch.from_numpy(data[:,6].astype('int32'))\n",
    "time2 = datetime.datetime.now()\n",
    "print(scene_points.shape)\n",
    "print(segment_label.shape)\n",
    "print(time2-time1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gen batch to hdf5 file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pointnet method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(data, num_sample):\n",
    "    \"\"\" data is in N x ...\n",
    "        we want to keep num_samplexC of them.\n",
    "        if N > num_sample, we will randomly keep num_sample of them.\n",
    "        if N < num_sample, we will randomly duplicate samples.\n",
    "    \"\"\"\n",
    "    N = data.shape[0]\n",
    "    if (N == num_sample):\n",
    "        return data, range(N)\n",
    "    elif (N > num_sample):\n",
    "        sample = np.random.choice(N, num_sample)\n",
    "        return data[sample, ...], sample\n",
    "    else:\n",
    "        sample = np.random.choice(N, num_sample-N)\n",
    "        dup_data = data[sample, ...]\n",
    "        return np.concatenate([data, dup_data], 0), list(range(N))+list(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data_label(data, label, num_sample):\n",
    "    new_data, sample_indices = sample_data(data, num_sample)\n",
    "    new_label = label[sample_indices]\n",
    "    return new_data, new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\n",
      "[1000, 4096, 3]\n",
      "(1000, 4096, 3)\n",
      "(1000, 4096)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "NUM_POINT = 4096\n",
    "H5_BATCH_SIZE = 1000\n",
    "data_dim = [NUM_POINT, 3]\n",
    "label_dim = [NUM_POINT]\n",
    "\n",
    "batch_data_dim = [H5_BATCH_SIZE] + data_dim\n",
    "batch_label_dim = [H5_BATCH_SIZE] + label_dim\n",
    "h5_batch_data = np.zeros(batch_data_dim, dtype = np.float32)\n",
    "h5_batch_label = np.zeros(batch_label_dim, dtype = np.uint8)\n",
    "buffer_size = 0  # state: record how many samples are currently in buffer\n",
    "h5_index = 0 # state: the next h5 file to save\n",
    "\n",
    "print([H5_BATCH_SIZE])\n",
    "print(batch_data_dim)\n",
    "print(h5_batch_data.shape)\n",
    "print(h5_batch_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2048, 3)\n"
     ]
    }
   ],
   "source": [
    "data_size = current_data.shape[0]\n",
    "h5_batch_data[buffer_size:buffer_size+data_size, ...] = current_data\n",
    "buffer_size += data_size\n",
    "\n",
    "print(h5_batch_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate hdf5 format (1000, 2048, 10) point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/7_SMV_chapel_24.txt\n",
      "input file: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/train/7_SMV_chapel_24.txt\n",
      "input scene: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/train/7_SMV_chapel_24.txt and shape: (3571064, 7)\n",
      "block number: 352, 8425\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"pre_hdf5/gen_arch_h5_pointnet.py\", line 122, in <module>\n",
      "    data, labels = common.scene2blocks_wrapper(filepath,NUM_POINT, block_size=args.block_size, stride=args.stride, random_sample=False, sample_num=None)\n",
      "  File \"/home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/pre_hdf5/common.py\", line 164, in scene2blocks_wrapper\n",
      "    random_sample, sample_num, sample_aug)\n",
      "  File \"/home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/pre_hdf5/common.py\", line 155, in scene2blocks_plus\n",
      "    random_sample, sample_num, sample_aug)\n",
      "  File \"/home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/pre_hdf5/common.py\", line 130, in scene2blocks\n",
      "    if np.sum(cond) < 100: # discard block if there are less than 100 pts.\n",
      "  File \"<__array_function__ internals>\", line 6, in sum\n",
      "  File \"/home/yw/anaconda3/envs/PyTorch1.2/lib/python3.7/site-packages/numpy/core/fromnumeric.py\", line 2242, in sum\n",
      "    initial=initial, where=where)\n",
      "  File \"/home/yw/anaconda3/envs/PyTorch1.2/lib/python3.7/site-packages/numpy/core/fromnumeric.py\", line 87, in _wrapreduction\n",
      "    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "#generate h5 files for scene 7\n",
    "!python pre_hdf5/gen_arch_h5_pointnet.py --split_file '7_SMV_chapel_24.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate h5 files for test data\n",
    "!python pre_hdf5/gen_arch_h5_pointnet.py --split 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test PointCNN point clouds split algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(block_size=1.5, folder=None, grid_size=0.03, max_point_num=8192, random_sample=False, save_ply=False)\n",
      "2020-11-24 22:02:09.794724-Loading /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/test/A_SMG_portico.txt...\n",
      "2020-11-24 22:06:16.629737-Computing block id of 17798012 points...\n",
      "2020-11-24 22:06:19.403253-Collecting points belong to each block...\n",
      "2020-11-24 22:06:47.949717-A_SMG_portico.txt is split into 432 blocks.\n",
      "2020-11-24 22:06:47.956153-32 of 432 blocks are merged.\n",
      "2020-11-24 22:07:53.162339-Saving /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/test/A_SMG_portico.txt_8196_zero_0.h5...\n",
      "2020-11-24 22:07:58.871601-Saving /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/test/A_SMG_portico.txt_8196_zero_1.h5...\n",
      "2020-11-24 22:08:02.120558-Computing block id of 17798012 points...\n",
      "2020-11-24 22:08:04.733596-Collecting points belong to each block...\n",
      "2020-11-24 22:08:32.386394-A_SMG_portico.txt is split into 432 blocks.\n",
      "2020-11-24 22:08:32.390463-30 of 432 blocks are merged.\n",
      "2020-11-24 22:09:35.842944-Saving /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/test/A_SMG_portico.txt_8196_half_0.h5...\n",
      "2020-11-24 22:09:42.378493-Saving /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/test/A_SMG_portico.txt_8196_half_1.h5...\n",
      "2020-11-24 22:09:45.232660-Loading /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/test/B_SMV_chapel_27to35.txt...\n",
      "2020-11-24 22:13:27.463520-Computing block id of 16200442 points...\n",
      "2020-11-24 22:13:29.374416-Collecting points belong to each block...\n",
      "2020-11-24 22:13:55.128973-B_SMV_chapel_27to35.txt is split into 296 blocks.\n",
      "2020-11-24 22:13:55.131813-9 of 296 blocks are merged.\n",
      "2020-11-24 22:14:53.677671-Saving /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/test/B_SMV_chapel_27to35.txt_8196_zero_0.h5...\n",
      "2020-11-24 22:14:59.603905-Saving /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/test/B_SMV_chapel_27to35.txt_8196_zero_1.h5...\n",
      "2020-11-24 22:15:01.033022-Computing block id of 16200442 points...\n",
      "2020-11-24 22:15:03.084392-Collecting points belong to each block...\n",
      "2020-11-24 22:15:28.964362-B_SMV_chapel_27to35.txt is split into 314 blocks.\n",
      "2020-11-24 22:15:28.965919-7 of 314 blocks are merged.\n",
      "2020-11-24 22:16:27.813992-Saving /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/test/B_SMV_chapel_27to35.txt_8196_half_0.h5...\n",
      "2020-11-24 22:16:33.947267-Saving /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/test/B_SMV_chapel_27to35.txt_8196_half_1.h5...\n",
      "2020-11-24 22:16:35.240724-Loading /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/train_scene7/7_SMV_chapel_24.txt...\n",
      "2020-11-24 22:17:27.634707-Computing block id of 3571064 points...\n",
      "2020-11-24 22:17:28.044490-Collecting points belong to each block...\n",
      "2020-11-24 22:17:33.083554-7_SMV_chapel_24.txt is split into 92 blocks.\n",
      "2020-11-24 22:17:33.084288-8 of 92 blocks are merged.\n",
      "2020-11-24 22:17:49.018582-Saving /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/train_scene7/7_SMV_chapel_24.txt_8196_zero_0.h5...\n",
      "2020-11-24 22:17:49.211798-Computing block id of 3571064 points...\n",
      "2020-11-24 22:17:49.623180-Collecting points belong to each block...\n",
      "2020-11-24 22:17:54.713086-7_SMV_chapel_24.txt is split into 95 blocks.\n",
      "2020-11-24 22:17:54.713653-4 of 95 blocks are merged.\n",
      "2020-11-24 22:18:10.941305-Saving /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/train_scene7/7_SMV_chapel_24.txt_8196_half_0.h5...\n",
      "2020-11-24 22:18:11.150550-Done.\n"
     ]
    }
   ],
   "source": [
    "!python pre_hdf5/gen_arch_h5_pointcnn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5(h5_filename):\n",
    "    f = h5py.File(h5_filename, 'r')\n",
    "    data = f['data'][:]\n",
    "    label = f['label'][:]\n",
    "    data_num = f['data_num'][...].astype(np.int32)\n",
    "    labels_seg = f['label_seg'][...].astype(np.int64)\n",
    "    if 'indices_split_to_full' in f:\n",
    "        indices_split_to_full = f['indices_split_to_full'][...].astype(np.int64)\n",
    "    return (data, label, data_num, labels_seg, indices_split_to_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 8196, 6)\n",
      "(96, 8196)\n",
      "(96,)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "data_train, _, data_num_train, label_train, _ = load_h5(\"./data/arch/Train1/12_KAS_pavillion_1.txt_8192_half_0.h5\")\n",
    "print(data_train.shape)\n",
    "print(label_train.shape)\n",
    "print(data_num_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test load data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_seg(filelist):\n",
    "    points = []\n",
    "    labels = []\n",
    "    point_nums = []\n",
    "    labels_seg = []\n",
    "    indices_split_to_full = []\n",
    "\n",
    "    for fn in filelist:\n",
    "        data = h5py.File(fn, 'r')\n",
    "        points.append(data['data'][...].astype(np.float32))\n",
    "        labels.append(data['label'][...].astype(np.int64))\n",
    "        point_nums.append(data['data_num'][...].astype(np.int32))\n",
    "        labels_seg.append(data['label_seg'][...].astype(np.int64))\n",
    "        if 'indices_split_to_full' in data:\n",
    "            indices_split_to_full.append(data['indices_split_to_full'][...].astype(np.int64))\n",
    "\n",
    "    return (np.concatenate(points, axis=0),\n",
    "            np.concatenate(labels, axis=0),\n",
    "            np.concatenate(point_nums, axis=0),\n",
    "            np.concatenate(labels_seg, axis=0),\n",
    "            np.concatenate(indices_split_to_full, axis=0) if indices_split_to_full else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-29 17:36:45.085651-Preparing datasets...\n",
      "./data/arch/./filelists/train_files_g_0.txt\n",
      "(8192, 2048, 6)\n",
      "(8192,)\n",
      "(8192, 2048)\n",
      "2020-10-29 17:36:45.708164-8192 training samples.\n",
      "2020-10-29 17:36:45.708325-174763 training batches.\n"
     ]
    }
   ],
   "source": [
    "# Prepare inputs\n",
    "import os.path\n",
    "import sys\n",
    "from datetime import datetime\n",
    "sys.path.append(os.path.join(\".\", 'utils'))\n",
    "import pc_utils\n",
    "\n",
    "\n",
    "filelist = \"./data/arch/train_data_files.txt\"\n",
    "batch_size = 12\n",
    "num_epochs = 256\n",
    "print('{}-Preparing datasets...'.format(datetime.now()))\n",
    "is_list_of_h5_list = not pc_utils.is_h5_list(filelist)\n",
    "if is_list_of_h5_list:\n",
    "    seg_list = pc_utils.load_seg_list(filelist)\n",
    "    seg_list_idx = 0\n",
    "    filelist_train = seg_list[seg_list_idx]\n",
    "    seg_list_idx = seg_list_idx + 1\n",
    "else:\n",
    "    filelist_train =  filelist\n",
    "print(filelist_train)\n",
    "data_train, _, data_num_train, label_train, _ = pc_utils.load_seg(filelist_train)\n",
    "print(data_train.shape)\n",
    "print(data_num_train.shape)\n",
    "print(label_train.shape)\n",
    "num_train = data_train.shape[0]\n",
    "point_num = data_train.shape[1]\n",
    "\n",
    "print('{}-{:d} training samples.'.format(datetime.now(), num_train))\n",
    "batch_num = (num_train * num_epochs + batch_size - 1) // batch_size\n",
    "print('{}-{:d} training batches.'.format(datetime.now(), batch_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = \"data/arch/train_data_files_1.txt\"\n",
    "path_h5py_all = [line.strip()for line in open(filelist)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./datasets/dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=16, dataset='shapenetcorev2', dropout=0.5, encoder='foldnet', epochs=10, eval=False, experiment_name=None, feat_dims=512, gpu_mode=False, k=None, model_path='', num_points=2048, num_workers=0, snapshot_interval=10, split='train', use_jitter=False, use_rotate=False, use_translate=False, workers=16)\n",
      "-Preparing dataset...\n",
      "-Loading ShapeNetCore dataset...\n",
      "(2048, 2048, 3)\n",
      "training set size:  100\n",
      "training start!!!\n",
      "total training nuber: 100total batch number: 6 .\n",
      "batch idx: 0/6 in 0/10 epoch...\n",
      "batch idx: 1/6 in 0/10 epoch...\n",
      "batch idx: 2/6 in 0/10 epoch...\n",
      "batch idx: 3/6 in 0/10 epoch...\n",
      "batch idx: 4/6 in 0/10 epoch...\n",
      "batch idx: 5/6 in 0/10 epoch...\n",
      "batch idx: 6/6 in 0/10 epoch...\n",
      "Epoch 1: Loss 10304.005859375, time 34.5129s\n",
      "Save model to snapshort/Reconstruct1106233149/models/shapenetcorev2_1.pkl\n",
      "Save model to snapshort/Reconstruct1106233149/models/shapenetcorev2_best.pkl\n",
      "end epoch 0, training loss: 10304.006\n",
      "total training nuber: 100total batch number: 6 .\n",
      "batch idx: 0/6 in 1/10 epoch...\n",
      "batch idx: 1/6 in 1/10 epoch...\n",
      "batch idx: 2/6 in 1/10 epoch...\n",
      "batch idx: 3/6 in 1/10 epoch...\n",
      "batch idx: 4/6 in 1/10 epoch...\n",
      "batch idx: 5/6 in 1/10 epoch...\n",
      "batch idx: 6/6 in 1/10 epoch...\n",
      "Epoch 2: Loss 10304.005859375, time 34.7295s\n",
      "end epoch 1, training loss: 10304.006\n",
      "total training nuber: 100total batch number: 6 .\n",
      "batch idx: 0/6 in 2/10 epoch...\n",
      "batch idx: 1/6 in 2/10 epoch...\n",
      "batch idx: 2/6 in 2/10 epoch...\n",
      "batch idx: 3/6 in 2/10 epoch...\n",
      "batch idx: 4/6 in 2/10 epoch...\n",
      "batch idx: 5/6 in 2/10 epoch...\n",
      "batch idx: 6/6 in 2/10 epoch...\n",
      "Epoch 3: Loss 10304.0048828125, time 36.2974s\n",
      "end epoch 2, training loss: 10304.005\n",
      "total training nuber: 100total batch number: 6 .\n",
      "batch idx: 0/6 in 3/10 epoch...\n",
      "batch idx: 1/6 in 3/10 epoch...\n",
      "batch idx: 2/6 in 3/10 epoch...\n",
      "batch idx: 3/6 in 3/10 epoch...\n",
      "batch idx: 4/6 in 3/10 epoch...\n",
      "batch idx: 5/6 in 3/10 epoch...\n",
      "batch idx: 6/6 in 3/10 epoch...\n",
      "Epoch 4: Loss 10304.0068359375, time 36.1685s\n",
      "end epoch 3, training loss: 10304.007\n",
      "total training nuber: 100total batch number: 6 .\n",
      "batch idx: 0/6 in 4/10 epoch...\n",
      "batch idx: 1/6 in 4/10 epoch...\n",
      "batch idx: 2/6 in 4/10 epoch...\n",
      "batch idx: 3/6 in 4/10 epoch...\n",
      "batch idx: 4/6 in 4/10 epoch...\n",
      "batch idx: 5/6 in 4/10 epoch...\n",
      "batch idx: 6/6 in 4/10 epoch...\n",
      "Epoch 5: Loss 10304.0048828125, time 35.5588s\n",
      "end epoch 4, training loss: 10304.005\n",
      "total training nuber: 100total batch number: 6 .\n",
      "batch idx: 0/6 in 5/10 epoch...\n",
      "batch idx: 1/6 in 5/10 epoch...\n",
      "batch idx: 2/6 in 5/10 epoch...\n",
      "batch idx: 3/6 in 5/10 epoch...\n",
      "batch idx: 4/6 in 5/10 epoch...\n",
      "batch idx: 5/6 in 5/10 epoch...\n",
      "batch idx: 6/6 in 5/10 epoch...\n",
      "Epoch 6: Loss 10304.005859375, time 35.8643s\n",
      "end epoch 5, training loss: 10304.006\n",
      "total training nuber: 100total batch number: 6 .\n",
      "batch idx: 0/6 in 6/10 epoch...\n",
      "batch idx: 1/6 in 6/10 epoch...\n",
      "batch idx: 2/6 in 6/10 epoch...\n",
      "batch idx: 3/6 in 6/10 epoch...\n",
      "batch idx: 4/6 in 6/10 epoch...\n",
      "batch idx: 5/6 in 6/10 epoch...\n",
      "batch idx: 6/6 in 6/10 epoch...\n",
      "Epoch 7: Loss 10304.0078125, time 35.6275s\n",
      "end epoch 6, training loss: 10304.008\n",
      "total training nuber: 100total batch number: 6 .\n",
      "batch idx: 0/6 in 7/10 epoch...\n",
      "batch idx: 1/6 in 7/10 epoch...\n",
      "batch idx: 2/6 in 7/10 epoch...\n",
      "batch idx: 3/6 in 7/10 epoch...\n",
      "batch idx: 4/6 in 7/10 epoch...\n",
      "batch idx: 5/6 in 7/10 epoch...\n",
      "batch idx: 6/6 in 7/10 epoch...\n",
      "Epoch 8: Loss 10304.005859375, time 35.6104s\n",
      "end epoch 7, training loss: 10304.006\n",
      "total training nuber: 100total batch number: 6 .\n",
      "batch idx: 0/6 in 8/10 epoch...\n",
      "batch idx: 1/6 in 8/10 epoch...\n",
      "batch idx: 2/6 in 8/10 epoch...\n",
      "batch idx: 3/6 in 8/10 epoch...\n",
      "batch idx: 4/6 in 8/10 epoch...\n",
      "batch idx: 5/6 in 8/10 epoch...\n",
      "batch idx: 6/6 in 8/10 epoch...\n",
      "Epoch 9: Loss 10304.0068359375, time 35.6151s\n",
      "end epoch 8, training loss: 10304.007\n",
      "total training nuber: 100total batch number: 6 .\n",
      "batch idx: 0/6 in 9/10 epoch...\n",
      "batch idx: 1/6 in 9/10 epoch...\n",
      "batch idx: 2/6 in 9/10 epoch...\n",
      "batch idx: 3/6 in 9/10 epoch...\n",
      "batch idx: 4/6 in 9/10 epoch...\n",
      "batch idx: 5/6 in 9/10 epoch...\n",
      "batch idx: 6/6 in 9/10 epoch...\n",
      "Epoch 10: Loss 10304.0048828125, time 35.8865s\n",
      "Save model to snapshort/Reconstruct1106233149/models/shapenetcorev2_10.pkl\n",
      "Save model to snapshort/Reconstruct1106233149/models/shapenetcorev2_best.pkl\n",
      "end epoch 9, training loss: 10304.005\n",
      "Save model to snapshort/Reconstruct1106233149/models/shapenetcorev2_10.pkl\n",
      "Avg one epoch time: 35.59, total 10 epoches time: 356.23\n",
      "Training finish!... save training results\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=16, dataset='shapenetcorev2', dropout=0.5, encoder='foldnet', epochs=10, eval=True, experiment_name=None, feat_dims=512, gpu_mode=False, k=None, model_path='./snapshort/Reconstruct1106233149/models/shapenetcorev2_best.pkl', num_points=2048, num_workers=0, snapshot_interval=10, split='train', use_jitter=False, use_rotate=False, use_translate=False, workers=16)\n",
      "-Preparing evaluation dataset...\n",
      "-Preparing ShapeNetCore evaluation dataset...\n",
      "(2048, 2048, 3)\n",
      "training set size: 100\n",
      "(2048, 2048, 3)\n",
      "testing set size: 100\n",
      "Load model from ./snapshort/Reconstruct1106233149/models/shapenetcorev2_best.pkl.\n",
      "size of generate traing set: (100, 512) ,(100,)\n",
      "Train set 0 for SVM saved.\n",
      "finish generating train set for SVM.\n",
      "batch idx: 0 for generating test set for SVM...\n",
      "batch idx: 1 for generating test set for SVM...\n",
      "batch idx: 2 for generating test set for SVM...\n",
      "batch idx: 3 for generating test set for SVM...\n",
      "batch idx: 4 for generating test set for SVM...\n",
      "batch idx: 5 for generating test set for SVM...\n",
      "batch idx: 6 for generating test set for SVM...\n",
      "size of generate test set: (100, 512) ,(100,)\n",
      "Test set 0 for SVM saved.\n",
      "finish generating test set for SVM.\n",
      "Loading feature dataset...\n",
      "Training set size: 100\n",
      "Testing set size: 100\n",
      "Transfer linear SVM accuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "!python main.py --eval --model_path \"./snapshort/Reconstruct1106233149/models/shapenetcorev2_best.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "TensorBoard 2.3.0 at http://yw:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir tensorboard --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataset='shapenetcorev2', dataset_root='./data', draw_original=True, draw_source_points=True, encoder='foldingnet', exp_name=None, feat_dims=512, item=1, k=None, model_path='snapshot/Reconstruct1106233149/models', num_points=2048, split='train')\n",
      "Dataset: shapenetcorev2, split: train, item: 1, category: car\n"
     ]
    }
   ],
   "source": [
    "!python visualization.py --item 1 --model_path=snapshot/Reconstruct1106233149/models --draw_original --draw_source_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source /home/yw/Downloads/mitsuba2/setpath.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: mitsuba: command not found\n"
     ]
    }
   ],
   "source": [
    "!mitsuba \"mitsuba/Reconstruct1106233149/shapenetcorev2/train1/shapenetcorev2_train1_car_epoch0.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
