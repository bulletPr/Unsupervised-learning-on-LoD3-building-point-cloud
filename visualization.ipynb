{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare time of different read data methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pptk\n",
    "import datetime\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use numpy ndarray read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15740229, 3)\n",
      "(15740229,)\n",
      "0:03:34.670069\n"
     ]
    }
   ],
   "source": [
    "file = \"./data/arch/Train/1_TR_cloister.txt\"\n",
    "time3 = datetime.datetime.now()\n",
    "data = np.loadtxt(file)\n",
    "scene_points = data[:,0:3].astype('float32')\n",
    "segment_label = data[:,6].astype('int64')\n",
    "time4 = datetime.datetime.now()\n",
    "print(scene_points.shape)\n",
    "print(segment_label.shape)\n",
    "print(time4-time3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'segment_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-1d324092b148>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpoint_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscene_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'segment_label' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "choice = np.random.choice(len(segment_label), 1024, replace=True)\n",
    "point_set = scene_points[choice, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use numpy read file + use torch read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15740229, 3])\n",
      "torch.Size([15740229])\n",
      "0:03:52.921357\n"
     ]
    }
   ],
   "source": [
    "file = \"./data/arch/Train/1_TR_cloister.txt\"\n",
    "time1 = datetime.datetime.now()\n",
    "data = np.loadtxt(file)\n",
    "scene_points = torch.from_numpy(data[:, 0:3].astype('float32'))\n",
    "segment_label = torch.from_numpy(data[:,6].astype('int32'))\n",
    "time2 = datetime.datetime.now()\n",
    "print(scene_points.shape)\n",
    "print(segment_label.shape)\n",
    "print(time2-time1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Block - random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 2048)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(data, num_sample):\n",
    "    \"\"\" data is in N x ...\n",
    "        we want to keep num_samplexC of them.\n",
    "        if N > num_sample, we will randomly keep num_sample of them.\n",
    "        if N < num_sample, we will randomly duplicate samples.\n",
    "    \"\"\"\n",
    "    N = data.shape[0]\n",
    "    if (N == num_sample):\n",
    "        return data, range(N)\n",
    "    elif (N > num_sample):\n",
    "        sample = np.random.choice(N, num_sample)\n",
    "        return data[sample, ...], sample\n",
    "    else:\n",
    "        sample = np.random.choice(N, num_sample-N)\n",
    "        dup_data = data[sample, ...]\n",
    "        return np.concatenate([data, dup_data], 0), list(range(N))+list(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data_label(data, label, num_sample):\n",
    "    new_data, sample_indices = sample_data(data, num_sample)\n",
    "    new_label = label[sample_indices]\n",
    "    return new_data, new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35296084, 0.72715114, 0.40660786],\n",
       "       [0.79695103, 0.35178304, 0.1483732 ],\n",
       "       [0.02184964, 0.89369391, 0.21384761],\n",
       "       ...,\n",
       "       [0.72761692, 0.63170081, 0.70812537],\n",
       "       [0.58366183, 0.69593253, 0.49909007],\n",
       "       [0.46816175, 0.91908899, 0.4233715 ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.random.random((2080,3))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [1],\n",
       "       [0],\n",
       "       ...,\n",
       "       [3],\n",
       "       [5],\n",
       "       [7]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = np.random.randint(0,10,size=[2080,1])\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_data_sampled, block_label_sampled = sample_data_label(data, label, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 3)\n",
      "(2048, 1)\n"
     ]
    }
   ],
   "source": [
    "print(block_data_sampled.shape)\n",
    "print(block_label_sampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.364098  , 0.30908428, 0.63440443],\n",
       "       [0.30548676, 0.40795002, 0.67965822],\n",
       "       [0.52483172, 0.76652738, 0.86369489],\n",
       "       ...,\n",
       "       [0.16839643, 0.47371034, 0.72023846],\n",
       "       [0.05214054, 0.68377316, 0.61206864],\n",
       "       [0.47624035, 0.64036852, 0.56606007]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = np.random.random((2000,3))\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2 = np.random.randint(0,10,size=[2000,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_data2_sampled, block_label2_sampled = sample_data_label(data2, label2, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 3)\n",
      "(2048, 1)\n"
     ]
    }
   ],
   "source": [
    "print(block_data2_sampled.shape)\n",
    "print(block_label2_sampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93, 2048, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_data = np.tile(block_data2_sampled, (93,1,1))\n",
    "current_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "(11, 8, 2048, 3)\n"
     ]
    }
   ],
   "source": [
    "file_size = current_data.shape[0]\n",
    "batch_size = 8\n",
    "num_batches = file_size//batch_size\n",
    "all_data = []\n",
    "\n",
    "print(num_batches)\n",
    "for batch_idx in range(num_batches):\n",
    "    #if num_batches == file_size\n",
    "    start_idx = batch_idx*8\n",
    "    end_idx = (batch_idx+1)*8\n",
    "    all_data.append(current_data[start_idx:end_idx, :, :])\n",
    "all_data = np.array(all_data)\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test block - gen batch to hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\n",
      "[1000, 2048, 3]\n",
      "(1000, 2048, 3)\n",
      "(1000, 2048)\n"
     ]
    }
   ],
   "source": [
    "NUM_POINT = 2048\n",
    "H5_BATCH_SIZE = 1000\n",
    "data_dim = [NUM_POINT, 3]\n",
    "label_dim = [NUM_POINT]\n",
    "\n",
    "batch_data_dim = [H5_BATCH_SIZE] + data_dim\n",
    "batch_label_dim = [H5_BATCH_SIZE] + label_dim\n",
    "h5_batch_data = np.zeros(batch_data_dim, dtype = np.float32)\n",
    "h5_batch_label = np.zeros(batch_label_dim, dtype = np.uint8)\n",
    "buffer_size = 0  # state: record how many samples are currently in buffer\n",
    "h5_index = 0 # state: the next h5 file to save\n",
    "\n",
    "print([H5_BATCH_SIZE])\n",
    "print(batch_data_dim)\n",
    "print(h5_batch_data.shape)\n",
    "print(h5_batch_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2048, 3)\n"
     ]
    }
   ],
   "source": [
    "data_size = current_data.shape[0]\n",
    "h5_batch_data[buffer_size:buffer_size+data_size, ...] = current_data\n",
    "buffer_size += data_size\n",
    "\n",
    "print(h5_batch_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.36409798, 0.3090843 , 0.6344044 ],\n",
       "        [0.30548677, 0.40795   , 0.67965823],\n",
       "        [0.5248317 , 0.76652735, 0.8636949 ],\n",
       "        ...,\n",
       "        [0.20645209, 0.01392364, 0.14218394],\n",
       "        [0.22136463, 0.86750555, 0.24099368],\n",
       "        [0.46171084, 0.4232767 , 0.58267117]],\n",
       "\n",
       "       [[0.36409798, 0.3090843 , 0.6344044 ],\n",
       "        [0.30548677, 0.40795   , 0.67965823],\n",
       "        [0.5248317 , 0.76652735, 0.8636949 ],\n",
       "        ...,\n",
       "        [0.20645209, 0.01392364, 0.14218394],\n",
       "        [0.22136463, 0.86750555, 0.24099368],\n",
       "        [0.46171084, 0.4232767 , 0.58267117]],\n",
       "\n",
       "       [[0.36409798, 0.3090843 , 0.6344044 ],\n",
       "        [0.30548677, 0.40795   , 0.67965823],\n",
       "        [0.5248317 , 0.76652735, 0.8636949 ],\n",
       "        ...,\n",
       "        [0.20645209, 0.01392364, 0.14218394],\n",
       "        [0.22136463, 0.86750555, 0.24099368],\n",
       "        [0.46171084, 0.4232767 , 0.58267117]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5_batch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate hdf5 format (1000, 2048, 10) point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " common.py  'experiment updating.md'   __pycache__   utils\n",
      " data\t     LOG\t\t       README.md     visualization.ipynb\n",
      " datasets    model\t\t       sampling      visualization.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/datasets\n"
     ]
    }
   ],
   "source": [
    "cd datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArCH.py        gen_arch_h5.py  ShapeNetCore.py\n",
      "dataloader.py  __pycache__     synsetoffset2category.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input file: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/1_TR_cloister.txt\n",
      "input scene: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/1_TR_cloister.txt and shape: (15740229, 7)\n",
      "block number: 77, 38\n",
      "block data list size: 693\n",
      "(693, 2048, 6), (693, 2048)\n",
      "output file size: (693, 2048, 6), (693, 2048)\n",
      "sample number now: 693now insert_batch\n",
      "now in enough space location, store data in memory\n",
      "finish 0 times\n",
      "input file: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/15_OTT_church.txt\n",
      "input scene: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/15_OTT_church.txt and shape: (13302903, 7)\n",
      "block number: 82, 118\n",
      "block data list size: 708\n",
      "(708, 2048, 6), (708, 2048)\n",
      "output file size: (708, 2048, 6), (708, 2048)\n",
      "sample number now: 1401now insert_batch\n",
      "Stored /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch_hdf5_data/train/ply_data_all_0.h5 with size 1000\n",
      "now insert rest data to h5 batch again(401, 2048, 6)\n",
      "now in enough space location, store data in memory\n",
      "finish 1 times\n",
      "input file: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/10_SStefano_portico_1.txt\n",
      "input scene: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/10_SStefano_portico_1.txt and shape: (3783699, 7)\n",
      "block number: 50, 43\n",
      "block data list size: 380\n",
      "(380, 2048, 6), (380, 2048)\n",
      "output file size: (380, 2048, 6), (380, 2048)\n",
      "sample number now: 1781now insert_batch\n",
      "now in enough space location, store data in memory\n",
      "finish 2 times\n",
      "input file: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/2_TR_church.txt\n",
      "input scene: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/2_TR_church.txt and shape: (20862139, 7)\n",
      "block number: 93, 46\n",
      "block data list size: 641\n",
      "(641, 2048, 6), (641, 2048)\n",
      "output file size: (641, 2048, 6), (641, 2048)\n",
      "sample number now: 2422now insert_batch\n",
      "Stored /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch_hdf5_data/train/ply_data_all_1.h5 with size 1000\n",
      "now insert rest data to h5 batch again(422, 2048, 6)\n",
      "now in enough space location, store data in memory\n",
      "finish 3 times\n",
      "input file: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/9_SMV_chapel_10.txt\n",
      "input scene: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/9_SMV_chapel_10.txt and shape: (2193189, 7)\n",
      "block number: 51, 62\n",
      "block data list size: 110\n",
      "(110, 2048, 6), (110, 2048)\n",
      "output file size: (110, 2048, 6), (110, 2048)\n",
      "sample number now: 2532now insert_batch\n",
      "now in enough space location, store data in memory\n",
      "finish 4 times\n",
      "input file: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/4_CA_church.txt\n",
      "input scene: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/4_CA_church.txt and shape: (4850807, 7)\n",
      "block number: 92, 36\n",
      "block data list size: 333\n",
      "(333, 2048, 6), (333, 2048)\n",
      "output file size: (333, 2048, 6), (333, 2048)\n",
      "sample number now: 2865now insert_batch\n",
      "now in enough space location, store data in memory\n",
      "finish 5 times\n",
      "input file: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/6_SMV_chapel_2to4.txt\n",
      "input scene: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/6_SMV_chapel_2to4.txt and shape: (6326871, 7)\n",
      "block number: 251, 4255\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"gen_arch_h5.py\", line 108, in <module>\n",
      "    stride=1.0)\n",
      "  File \"/home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/utils/common.py\", line 164, in scenetoblocks_wrapper\n",
      "    sampling, sample_num, sample_aug)\n",
      "  File \"/home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/utils/common.py\", line 155, in scenetoblocks_plus\n",
      "    sampling, sample_num, sample_aug)\n",
      "  File \"/home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/utils/common.py\", line 127, in scenetoblocks\n",
      "    xcond = (data[:,0]<=xbeg+block_size) & (data[:,0]>=xbeg)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python gen_arch_h5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "block_size=1.0\n",
    "stride = 1.0\n",
    "data_label_filename = \"/home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/14_TRE_square.txt\"\n",
    "data_label = np.loadtxt(data_label_filename)\n",
    "data = data_label[:,0:6]\n",
    "#get the corner location for our sampling blocks\n",
    "limit = np.amax(data,0)[0:3]\n",
    "\n",
    "#calculate number of blocks and add into block list\n",
    "xbeg_list = []\n",
    "ybeg_list = []\n",
    "\n",
    "num_block_x = int(np.ceil((limit[0] - block_size) / stride)) + 1\n",
    "num_block_y = int(np.ceil((limit[1] - block_size) / stride)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 78)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_block_x, num_block_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input file: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/2_TR_church.txt\n",
      "input scene: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/2_TR_church.txt and shape: (20862139, 7)\n",
      "block number: 93, 46\n",
      "block data list size: 641\n",
      "(641, 2048, 6), (641, 2048)\n",
      "output file size: (641, 2048, 6), (641, 2048)\n",
      "sample number now: 641now insert_batch\n",
      "now in enough space location, store data in memory\n",
      "finish 0 times\n",
      "input file: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/9_SMV_chapel_10.txt\n",
      "input scene: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/9_SMV_chapel_10.txt and shape: (2193189, 7)\n",
      "block number: 51, 62\n",
      "block data list size: 110\n",
      "(110, 2048, 6), (110, 2048)\n",
      "output file size: (110, 2048, 6), (110, 2048)\n",
      "sample number now: 751now insert_batch\n",
      "now in enough space location, store data in memory\n",
      "finish 1 times\n",
      "input file: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/4_CA_church.txt\n",
      "input scene: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/4_CA_church.txt and shape: (4850807, 7)\n",
      "block number: 92, 36\n",
      "block data list size: 333\n",
      "(333, 2048, 6), (333, 2048)\n",
      "output file size: (333, 2048, 6), (333, 2048)\n",
      "sample number now: 1084now insert_batch\n",
      "Stored /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch_hdf5_data/train/ply_data_all_2.h5 with size 1000\n",
      "now insert rest data to h5 batch again(84, 2048, 6)\n",
      "now in enough space location, store data in memory\n",
      "finish 2 times\n",
      "input file: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/12_KAS_pavillion_1.txt\n",
      "input scene: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/12_KAS_pavillion_1.txt and shape: (598380, 7)\n",
      "block number: 84, 57\n",
      "block data list size: 93\n",
      "(93, 2048, 6), (93, 2048)\n",
      "output file size: (93, 2048, 6), (93, 2048)\n",
      "sample number now: 1177now insert_batch\n",
      "now in enough space location, store data in memory\n",
      "finish 3 times\n",
      "input file: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/13_KAS_pavillion_2.txt\n",
      "input scene: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/13_KAS_pavillion_2.txt and shape: (325818, 7)\n",
      "block number: 84, 44\n",
      "block data list size: 48\n",
      "(48, 2048, 6), (48, 2048)\n",
      "output file size: (48, 2048, 6), (48, 2048)\n",
      "sample number now: 1225now insert_batch\n",
      "now in enough space location, store data in memory\n",
      "finish 4 times\n",
      "input file: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/14_TRE_square.txt\n",
      "input scene: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/14_TRE_square.txt and shape: (9409239, 7)\n",
      "block number: 62, 78\n",
      "block data list size: 450\n",
      "(450, 2048, 6), (450, 2048)\n",
      "output file size: (450, 2048, 6), (450, 2048)\n",
      "sample number now: 1675now insert_batch\n",
      "now in enough space location, store data in memory\n",
      "finish 5 times\n",
      "input file: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/11_SStefano_portico_2.txt\n",
      "input scene: /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch/Train1/11_SStefano_portico_2.txt and shape: (10047392, 7)\n",
      "block number: 47, 71\n",
      "block data list size: 11\n",
      "(11, 2048, 6), (11, 2048)\n",
      "output file size: (11, 2048, 6), (11, 2048)\n",
      "sample number now: 1686now insert_batch\n",
      "now in enough space location, store data in memory\n",
      "Stored /home/yw/Documents/experiment/Unsupervised-learning-on-LoD3-building-point-cloud/data/arch_hdf5_data/train/ply_data_all_3.h5 with size 686\n",
      "in last batch!\n",
      "finish 6 times\n",
      "Total samples: 1686\n"
     ]
    }
   ],
   "source": [
    "!python gen_arch_h5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test PointCNN point clouds split algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-28 17:20:54.410138-Loading ./data/arch/Test1/A_SMG_portico.txt...\n",
      "[('zero', 0.0), ('half', 1.0)]\n",
      "2020-10-28 17:24:46.661784-Computing block id of 17798012 points...\n",
      "xyz_min: [[ 69.39041138 103.79975128 374.05963135]]\n",
      "xyz_max: [[133.0887146  156.79910278 385.18084717]]\n",
      "11.121215819999975\n",
      "block_size: (2.0, 2.0, 22.24243163999995)\n",
      "[[29 22  0]\n",
      " [29 22  0]\n",
      " [29 22  0]\n",
      " ...\n",
      " [30 24  0]\n",
      " [30 24  0]\n",
      " [30 24  0]]\n",
      "2020-10-28 17:24:48.781681-Collecting points belong to each block...\n",
      "[[ 0  3  0]\n",
      " [ 0  4  0]\n",
      " [ 0  5  0]\n",
      " [ 0  6  0]\n",
      " [ 1  2  0]\n",
      " [ 1  3  0]\n",
      " [ 1  4  0]\n",
      " [ 1  5  0]\n",
      " [ 1  6  0]\n",
      " [ 1  7  0]\n",
      " [ 2  2  0]\n",
      " [ 2  3  0]\n",
      " [ 2  4  0]\n",
      " [ 2  5  0]\n",
      " [ 2  6  0]\n",
      " [ 2  7  0]\n",
      " [ 2  8  0]\n",
      " [ 2  9  0]\n",
      " [ 3  1  0]\n",
      " [ 3  2  0]\n",
      " [ 3  3  0]\n",
      " [ 3  4  0]\n",
      " [ 3  5  0]\n",
      " [ 3  6  0]\n",
      " [ 3  7  0]\n",
      " [ 3  8  0]\n",
      " [ 3  9  0]\n",
      " [ 3 10  0]\n",
      " [ 4  0  0]\n",
      " [ 4  1  0]\n",
      " [ 4  2  0]\n",
      " [ 4  3  0]\n",
      " [ 4  4  0]\n",
      " [ 4  5  0]\n",
      " [ 4  6  0]\n",
      " [ 4  7  0]\n",
      " [ 4  8  0]\n",
      " [ 4  9  0]\n",
      " [ 4 10  0]\n",
      " [ 4 11  0]\n",
      " [ 4 12  0]\n",
      " [ 5  0  0]\n",
      " [ 5  1  0]\n",
      " [ 5  2  0]\n",
      " [ 5  3  0]\n",
      " [ 5  4  0]\n",
      " [ 5  5  0]\n",
      " [ 5  6  0]\n",
      " [ 5  7  0]\n",
      " [ 5  8  0]\n",
      " [ 5  9  0]\n",
      " [ 5 10  0]\n",
      " [ 5 11  0]\n",
      " [ 5 12  0]\n",
      " [ 5 13  0]\n",
      " [ 6  0  0]\n",
      " [ 6  1  0]\n",
      " [ 6  2  0]\n",
      " [ 6  3  0]\n",
      " [ 6  4  0]\n",
      " [ 6  5  0]\n",
      " [ 6  6  0]\n",
      " [ 6  7  0]\n",
      " [ 6  8  0]\n",
      " [ 6  9  0]\n",
      " [ 6 10  0]\n",
      " [ 6 11  0]\n",
      " [ 6 12  0]\n",
      " [ 7  0  0]\n",
      " [ 7  1  0]\n",
      " [ 7  2  0]\n",
      " [ 7  3  0]\n",
      " [ 7  4  0]\n",
      " [ 7  5  0]\n",
      " [ 7  6  0]\n",
      " [ 7  7  0]\n",
      " [ 7  8  0]\n",
      " [ 7  9  0]\n",
      " [ 7 10  0]\n",
      " [ 7 11  0]\n",
      " [ 7 12  0]\n",
      " [ 8  0  0]\n",
      " [ 8  1  0]\n",
      " [ 8  2  0]\n",
      " [ 8  3  0]\n",
      " [ 8  4  0]\n",
      " [ 8  5  0]\n",
      " [ 8  6  0]\n",
      " [ 8  7  0]\n",
      " [ 8  8  0]\n",
      " [ 8  9  0]\n",
      " [ 8 10  0]\n",
      " [ 8 11  0]\n",
      " [ 9  1  0]\n",
      " [ 9  2  0]\n",
      " [ 9  3  0]\n",
      " [ 9  4  0]\n",
      " [ 9  5  0]\n",
      " [ 9  6  0]\n",
      " [ 9  7  0]\n",
      " [ 9  8  0]\n",
      " [ 9  9  0]\n",
      " [ 9 10  0]\n",
      " [10  2  0]\n",
      " [10  3  0]\n",
      " [10  4  0]\n",
      " [10  5  0]\n",
      " [10  6  0]\n",
      " [10  7  0]\n",
      " [10  8  0]\n",
      " [10  9  0]\n",
      " [10 10  0]\n",
      " [11  2  0]\n",
      " [11  3  0]\n",
      " [11  4  0]\n",
      " [11  5  0]\n",
      " [11  6  0]\n",
      " [11  7  0]\n",
      " [11  8  0]\n",
      " [11  9  0]\n",
      " [12  2  0]\n",
      " [12  3  0]\n",
      " [12  4  0]\n",
      " [12  5  0]\n",
      " [12  6  0]\n",
      " [12  7  0]\n",
      " [12  8  0]\n",
      " [13  2  0]\n",
      " [13  3  0]\n",
      " [13  4  0]\n",
      " [13  5  0]\n",
      " [13  6  0]\n",
      " [13  7  0]\n",
      " [13  8  0]\n",
      " [14  3  0]\n",
      " [14  4  0]\n",
      " [14  5  0]\n",
      " [14  6  0]\n",
      " [14  7  0]\n",
      " [14  8  0]\n",
      " [14  9  0]\n",
      " [15  5  0]\n",
      " [15  6  0]\n",
      " [15  7  0]\n",
      " [15  8  0]\n",
      " [15  9  0]\n",
      " [15 10  0]\n",
      " [16  6  0]\n",
      " [16  7  0]\n",
      " [16  8  0]\n",
      " [16  9  0]\n",
      " [16 10  0]\n",
      " [16 11  0]\n",
      " [17  7  0]\n",
      " [17  8  0]\n",
      " [17  9  0]\n",
      " [17 10  0]\n",
      " [17 11  0]\n",
      " [17 12  0]\n",
      " [17 13  0]\n",
      " [18  8  0]\n",
      " [18  9  0]\n",
      " [18 10  0]\n",
      " [18 11  0]\n",
      " [18 12  0]\n",
      " [18 13  0]\n",
      " [18 14  0]\n",
      " [19  9  0]\n",
      " [19 10  0]\n",
      " [19 11  0]\n",
      " [19 12  0]\n",
      " [19 13  0]\n",
      " [19 14  0]\n",
      " [19 15  0]\n",
      " [20 10  0]\n",
      " [20 11  0]\n",
      " [20 12  0]\n",
      " [20 13  0]\n",
      " [20 14  0]\n",
      " [20 15  0]\n",
      " [20 16  0]\n",
      " [21 11  0]\n",
      " [21 12  0]\n",
      " [21 13  0]\n",
      " [21 14  0]\n",
      " [21 15  0]\n",
      " [21 16  0]\n",
      " [21 17  0]\n",
      " [22 13  0]\n",
      " [22 14  0]\n",
      " [22 15  0]\n",
      " [22 16  0]\n",
      " [22 17  0]\n",
      " [22 18  0]\n",
      " [22 19  0]\n",
      " [23 14  0]\n",
      " [23 15  0]\n",
      " [23 16  0]\n",
      " [23 17  0]\n",
      " [23 18  0]\n",
      " [23 19  0]\n",
      " [23 20  0]\n",
      " [24 15  0]\n",
      " [24 16  0]\n",
      " [24 17  0]\n",
      " [24 18  0]\n",
      " [24 19  0]\n",
      " [24 20  0]\n",
      " [24 21  0]\n",
      " [25 16  0]\n",
      " [25 17  0]\n",
      " [25 18  0]\n",
      " [25 19  0]\n",
      " [25 20  0]\n",
      " [25 21  0]\n",
      " [25 22  0]\n",
      " [26 17  0]\n",
      " [26 18  0]\n",
      " [26 19  0]\n",
      " [26 20  0]\n",
      " [26 21  0]\n",
      " [26 22  0]\n",
      " [26 23  0]\n",
      " [27 18  0]\n",
      " [27 19  0]\n",
      " [27 20  0]\n",
      " [27 21  0]\n",
      " [27 22  0]\n",
      " [27 23  0]\n",
      " [27 24  0]\n",
      " [27 25  0]\n",
      " [28 20  0]\n",
      " [28 21  0]\n",
      " [28 22  0]\n",
      " [28 23  0]\n",
      " [28 24  0]\n",
      " [28 25  0]\n",
      " [28 26  0]\n",
      " [29 19  0]\n",
      " [29 20  0]\n",
      " [29 21  0]\n",
      " [29 22  0]\n",
      " [29 23  0]\n",
      " [29 24  0]\n",
      " [29 25  0]\n",
      " [29 26  0]\n",
      " [30 18  0]\n",
      " [30 19  0]\n",
      " [30 20  0]\n",
      " [30 21  0]\n",
      " [30 22  0]\n",
      " [30 23  0]\n",
      " [30 24  0]\n",
      " [30 25  0]\n",
      " [31 19  0]\n",
      " [31 20  0]\n",
      " [31 23  0]\n",
      " [31 24  0]\n",
      " [31 25  0]]\n",
      "[241 241 241 ... 252 252 252]\n",
      "[  2428  18423   7418   2196    401  40303  55579  21058  26557  16404\n",
      "  10494  81835  20788  60572  71459  81683  46702   1551   1815  77539\n",
      "  19824  17955  18019  59301  64245  23218  41881   9735     42  31143\n",
      "  42799  18653  18035  22123  77774  18607  19011  19308  20214  10006\n",
      "    441   8526  46274  18204  18772  19084  69493  22253  19347  19216\n",
      "  18979  18375  18185  21106    891  55196  17909  18432  18967  48239\n",
      "  36469  18920  19793  19847  19839  20112  25232  25165  48864  94338\n",
      "  33014  28615  68628  17275  18799  20502  20332  20471  19370  37739\n",
      "    937   1211  29065  55138  88072  17107  17364  19964  21553  22744\n",
      "  20002  32725   7311     81 128217  49277  17839  17234  22782  21420\n",
      "  19462  37885  33335  28537 128123  41035  49149  23473  18397  18738\n",
      "  18319    945   6183  15438 142519  41086  19009  21416  24731   5273\n",
      "  52544  64756  62091 136022  67276  39844  33369    138  93724  72133\n",
      "  77380 155843 150766   6392     10  93376 101624  89935 171063 215417\n",
      "  19015  37622 138755  90888 142276 219939  25252  25239 151357  85623\n",
      " 111587 227899  38271  13869 139562  88794  71868 193124  50909    197\n",
      "   7716 102201 116049  74075 167710  92818    769   1915 105187 104041\n",
      "  87713 185063 162689   4835    368 118691 107798  92483 157178 193025\n",
      "  30388      9  86177 103250  88430 148659 215618  43349  31455 152803\n",
      "  97800 134151 233428  54227      5  21159 171974 120274 118707 238741\n",
      "  72676     55  10629 142186 149398 106320 187821 102900    689   2149\n",
      "  96421 157626 110780 223455 156294   2672    137  86635 131015 119544\n",
      " 209450 205852  19770      7  94448 127781 114670 166598 260781 141145\n",
      "    310  52317 134415 116405 246160 330613 267671    558   4759  45797\n",
      "  57282 383210 121308  96576 373100 116811     90  34495  53347  27292\n",
      "  73251 330058 248656 157346  10923  33618  34344 146760    552]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d781e5fed9ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_point_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mblock_point_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint_block_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_point_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}-{} is split into {} blocks.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "filename_txt = \"./data/arch/Test1/A_SMG_portico.txt\"\n",
    "print('{}-Loading {}...'.format(datetime.now(), filename_txt))\n",
    "all_data = np.loadtxt(filename_txt)\n",
    "xyzrgbl = all_data[:, 0:7]\n",
    "labels = xyzrgbl[:,-1]\n",
    "\n",
    "xyz = xyzrgbl[:,0:3]\n",
    "rgb = xyzrgbl[:,3:6] / 255 - 0.5\n",
    "block_size = 2.0\n",
    "offsets = [('zero', 0.0), ('half', block_size / 2)]\n",
    "print(offsets)\n",
    "for offset_name, offset in offsets:\n",
    "    print('{}-Computing block id of {} points...'.format(datetime.now(), xyzrgbl.shape[0]))\n",
    "    xyz_min = np.amin(xyz, axis=0, keepdims=True) - offset\n",
    "    xyz_max = np.amax(xyz, axis=0, keepdims=True)\n",
    "    print(\"xyz_min: \" + str(xyz_min))\n",
    "    print(\"xyz_max: \" + str(xyz_max))\n",
    "    block_size = (block_size, block_size, 2 * (xyz_max[0, -1] - xyz_min[0, -1]))\n",
    "    print(xyz_max[0, -1] - xyz_min[0, -1])\n",
    "    print(\"block_size: \" + str(block_size))\n",
    "    xyz_blocks = np.floor((xyz - xyz_min) / block_size).astype(np.int)\n",
    "    print(xyz_blocks)\n",
    "    print('{}-Collecting points belong to each block...'.format(datetime.now(), xyzrgbl.shape[0]))\n",
    "    blocks, point_block_indices, block_point_counts = np.unique(xyz_blocks, return_inverse=True, return_counts=True, axis=0)\n",
    "    print(blocks)\n",
    "    print(point_block_indices)\n",
    "    print(block_point_counts)\n",
    "    block_point_indices = np.split(np.argsort(point_block_indices), np.cumsum(block_point_counts[:-1]))\n",
    "    print('{}-{} is split into {} blocks.'.format(datetime.now(), dataset, blocks.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5(h5_filename):\n",
    "    f = h5py.File(h5_filename, 'r')\n",
    "    data = f['data'][:]\n",
    "    label = f['label'][:]\n",
    "    data_num = f['data_num'][...].astype(np.int32)\n",
    "    labels_seg = f['label_seg'][...].astype(np.int64)\n",
    "    if 'indices_split_to_full' in f:\n",
    "        indices_split_to_full = f['indices_split_to_full'][...].astype(np.int64)\n",
    "    return (data, label, data_num, labels_seg, indices_split_to_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 8196, 6)\n",
      "(96, 8196)\n",
      "(96,)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "data_train, _, data_num_train, label_train, _ = load_h5(\"./data/arch/Train1/12_KAS_pavillion_1.txt_8192_half_0.h5\")\n",
    "print(data_train.shape)\n",
    "print(label_train.shape)\n",
    "print(data_num_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_seg(filelist):\n",
    "    points = []\n",
    "    labels = []\n",
    "    point_nums = []\n",
    "    labels_seg = []\n",
    "    indices_split_to_full = []\n",
    "\n",
    "    for fn in filelist:\n",
    "        data = h5py.File(fn, 'r')\n",
    "        points.append(data['data'][...].astype(np.float32))\n",
    "        labels.append(data['label'][...].astype(np.int64))\n",
    "        point_nums.append(data['data_num'][...].astype(np.int32))\n",
    "        labels_seg.append(data['label_seg'][...].astype(np.int64))\n",
    "        if 'indices_split_to_full' in data:\n",
    "            indices_split_to_full.append(data['indices_split_to_full'][...].astype(np.int64))\n",
    "\n",
    "    return (np.concatenate(points, axis=0),\n",
    "            np.concatenate(labels, axis=0),\n",
    "            np.concatenate(point_nums, axis=0),\n",
    "            np.concatenate(labels_seg, axis=0),\n",
    "            np.concatenate(indices_split_to_full, axis=0) if indices_split_to_full else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191, 8196, 6)\n",
      "(191, 8196)\n",
      "(191,)\n"
     ]
    }
   ],
   "source": [
    "filelist = [\"data/arch/Train1/12_KAS_pavillion_1.txt_8192_half_0.h5\",\"data/arch/Train1/12_KAS_pavillion_1.txt_8192_zero_0.h5\"]\n",
    "data_train, _, data_num_train, label_train, _ = load_seg(filelist)\n",
    "print(data_train.shape)\n",
    "print(label_train.shape)\n",
    "print(data_num_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-29 17:36:45.085651-Preparing datasets...\n",
      "./data/arch/./filelists/train_files_g_0.txt\n",
      "(8192, 2048, 6)\n",
      "(8192,)\n",
      "(8192, 2048)\n",
      "2020-10-29 17:36:45.708164-8192 training samples.\n",
      "2020-10-29 17:36:45.708325-174763 training batches.\n"
     ]
    }
   ],
   "source": [
    "# Prepare inputs\n",
    "import os.path\n",
    "import sys\n",
    "from datetime import datetime\n",
    "sys.path.append(os.path.join(\".\", 'utils'))\n",
    "import pc_utils\n",
    "\n",
    "\n",
    "filelist = \"./data/arch/train_data_files.txt\"\n",
    "batch_size = 12\n",
    "num_epochs = 256\n",
    "print('{}-Preparing datasets...'.format(datetime.now()))\n",
    "is_list_of_h5_list = not pc_utils.is_h5_list(filelist)\n",
    "if is_list_of_h5_list:\n",
    "    seg_list = pc_utils.load_seg_list(filelist)\n",
    "    seg_list_idx = 0\n",
    "    filelist_train = seg_list[seg_list_idx]\n",
    "    seg_list_idx = seg_list_idx + 1\n",
    "else:\n",
    "    filelist_train =  filelist\n",
    "print(filelist_train)\n",
    "data_train, _, data_num_train, label_train, _ = pc_utils.load_seg(filelist_train)\n",
    "print(data_train.shape)\n",
    "print(data_num_train.shape)\n",
    "print(label_train.shape)\n",
    "num_train = data_train.shape[0]\n",
    "point_num = data_train.shape[1]\n",
    "\n",
    "print('{}-{:d} training samples.'.format(datetime.now(), num_train))\n",
    "batch_num = (num_train * num_epochs + batch_size - 1) // batch_size\n",
    "print('{}-{:d} training batches.'.format(datetime.now(), batch_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !python ./datasets/dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
